{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf391fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff762ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedGaussianHMCSampler(TruncatedGaussianSamplerBase):\n",
    "    def __init__(self, mean, cov, lower, upper, step_size=0.01, n_steps=10, key=None):\n",
    "        super().__init__(mean, cov, lower, upper, key)\n",
    "        self.step_size = step_size\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def sample(self, n_samples, burn_in=100, x0=None):\n",
    "        if x0 is None:\n",
    "            x0 = self._initial_point()\n",
    "\n",
    "        def reflect(x, p):\n",
    "            for i in range(self.d):\n",
    "                out_lo = x[i] < self.lower[i]\n",
    "                out_hi = x[i] > self.upper[i]\n",
    "\n",
    "                x = x.at[i].set(jnp.where(out_lo, 2*self.lower[i] - x[i], x[i]))\n",
    "                x = x.at[i].set(jnp.where(out_hi, 2*self.upper[i] - x[i], x[i]))\n",
    "\n",
    "                p = p.at[i].set(jnp.where(out_lo | out_hi, -p[i], p[i]))\n",
    "\n",
    "            return x, p\n",
    "\n",
    "        def leapfrog(x, p):\n",
    "            p = p - 0.5 * self.step_size * (self.P @ (x - self.mean))\n",
    "            for _ in range(self.n_steps):\n",
    "                x = x + self.step_size * p\n",
    "                x, p = reflect(x, p)\n",
    "                p = p - self.step_size * (self.P @ (x - self.mean))\n",
    "            p = p + 0.5 * self.step_size * (self.P @ (x - self.mean))\n",
    "            return x\n",
    "\n",
    "        def hmc_step(x, key):\n",
    "            p = random.normal(key, (self.d,))\n",
    "            return leapfrog(x, p)\n",
    "\n",
    "        @partial(jit, static_argnums=(2,))\n",
    "        def _sample_impl(x0, key, n_samples):\n",
    "            def body(carry, _):\n",
    "                x, key = carry\n",
    "                key, subkey = random.split(key)\n",
    "                x = hmc_step(x, subkey)\n",
    "                return (x, key), x\n",
    "\n",
    "            (x0, key), _ = lax.scan(body, (x0, key), None, length=burn_in)\n",
    "            (_, _), xs = lax.scan(body, (x0, key), None, length=n_samples)\n",
    "            return xs\n",
    "\n",
    "        return _sample_impl(x0, self.key, n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap, lax\n",
    "from jax.scipy.linalg import cho_factor, cho_solve\n",
    "from functools import partial\n",
    "\n",
    "# Utility\n",
    "\n",
    "def norm_ppf(u):\n",
    "    return jnp.sqrt(2.0) * jax.scipy.special.erfinv(2.0 * u - 1.0)\n",
    "\n",
    "@jit\n",
    "def truncnorm_sample(key, a, b, shape=()):\n",
    "    Phi = 0.5 * (1 + jax.scipy.special.erf(a / jnp.sqrt(2)))\n",
    "    Phi_b = 0.5 * (1 + jax.scipy.special.erf(b / jnp.sqrt(2)))\n",
    "    u = random.uniform(key, shape=shape, minval=Phi, maxval=Phi_b)\n",
    "    return norm_ppf(u)\n",
    "\n",
    "class TruncatedGaussianSamplerBase:\n",
    "    def __init__(self, mean, cov, lower, upper, key=None):\n",
    "        self.mean = jnp.array(mean)\n",
    "        self.cov = jnp.array(cov)\n",
    "        self.lower = jnp.array(lower)\n",
    "        self.upper = jnp.array(upper)\n",
    "        self.d = self.mean.shape[0]\n",
    "        self.key = random.PRNGKey(0) if key is None else key\n",
    "        self.L = jnp.linalg.cholesky(self.cov)\n",
    "        cf = cho_factor(self.cov)\n",
    "        self.P = cho_solve(cf, jnp.eye(self.d))\n",
    "\n",
    "    def _initial_point(self):\n",
    "        return jnp.clip(self.mean, self.lower + 1e-6, self.upper - 1e-6)\n",
    "\n",
    "class TruncatedGaussianGibbsSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, burn_in=100, thinning=1, x0=None):\n",
    "        if x0 is None:\n",
    "            x0 = self._initial_point()\n",
    "\n",
    "        @partial(jit, static_argnums=(2, 3, 4))\n",
    "        def _sample_impl(x0, key, n_samples, burn_in, thinning):\n",
    "            def step(carry, _):\n",
    "                x, key = carry\n",
    "                keys = random.split(key, self.d + 1)\n",
    "                for i in range(self.d):\n",
    "                    pii = self.P[i, i]\n",
    "                    mu_i = self.mean[i] - (1.0 / pii) * (self.P[i] @ (x - self.mean))\n",
    "                    std_i = jnp.sqrt(1.0 / pii)\n",
    "                    a = (self.lower[i] - mu_i) / std_i\n",
    "                    b = (self.upper[i] - mu_i) / std_i\n",
    "                    z = truncnorm_sample(keys[i], a, b)\n",
    "                    x = x.at[i].set(mu_i + std_i * z)\n",
    "                return (x, keys[-1]), x\n",
    "\n",
    "            (x0, key), _ = lax.scan(step, (x0, key), None, length=burn_in)\n",
    "            (_, _), xs = lax.scan(step, (x0, key), None, length=n_samples * thinning)\n",
    "            return xs[thinning - 1::thinning]\n",
    "\n",
    "        return _sample_impl(x0, self.key, n_samples, burn_in, thinning)\n",
    "\n",
    "class TruncatedGaussianGHKSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, x0=None):\n",
    "        def _one_sample(key):\n",
    "            zs = jnp.zeros(self.d)\n",
    "            x = jnp.zeros(self.d)\n",
    "            for i in range(self.d):\n",
    "                key, sub = random.split(key)\n",
    "                mu_partial = self.mean[i] + (self.L[i, :i] @ zs[:i])\n",
    "                Li = self.L[i, i]\n",
    "                a = (self.lower[i] - mu_partial) / Li\n",
    "                b = (self.upper[i] - mu_partial) / Li\n",
    "                z = truncnorm_sample(sub, a, b)\n",
    "                zs = zs.at[i].set(z)\n",
    "                x = x.at[i].set(mu_partial + Li * z)\n",
    "            return x\n",
    "        return vmap(_one_sample)(random.split(self.key, n_samples))\n",
    "\n",
    "class TruncatedGaussianMinimaxTiltingSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, x0=None):\n",
    "        @partial(jit, static_argnums=(0,))\n",
    "        def _sample_impl(n_samples):\n",
    "            def body(carry, _):\n",
    "                samples, key = carry\n",
    "                key, sub = random.split(key)\n",
    "                z = random.normal(sub, (self.d,))\n",
    "                x = self.mean + self.L @ z\n",
    "                inside = jnp.all((x >= self.lower) & (x <= self.upper))\n",
    "                return (samples, key), jnp.where(inside, x, jnp.zeros_like(x))\n",
    "            (samples, _), out = lax.scan(body, (jnp.zeros((n_samples, self.d)), self.key), None, length=n_samples)\n",
    "            return out\n",
    "        return _sample_impl(n_samples)\n",
    "\n",
    "class TruncatedGaussianSliceSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, burn_in=100, thinning=1, x0=None):\n",
    "        if x0 is None:\n",
    "            x0 = self._initial_point()\n",
    "\n",
    "        @partial(jit, static_argnums=(2, 3, 4))\n",
    "        def _sample_impl(x0, key, n_samples, burn_in, thinning):\n",
    "            def update(carry, _):\n",
    "                x, key = carry\n",
    "                key, sub = random.split(key)\n",
    "                logp = -0.5 * (x - self.mean) @ self.P @ (x - self.mean)\n",
    "                key, u_key = random.split(key)\n",
    "                logy = logp + jnp.log(random.uniform(u_key))\n",
    "                for i in range(self.d):\n",
    "                    var = 1.0 / self.P[i, i]\n",
    "                    w = jnp.sqrt(var)\n",
    "                    L = jnp.maximum(self.lower[i], x[i] - w * random.uniform(key))\n",
    "                    R = jnp.minimum(self.upper[i], L + w)\n",
    "                    key, sub2 = random.split(key)\n",
    "                    x = x.at[i].set(random.uniform(sub2, minval=L, maxval=R))\n",
    "                return (x, key), x\n",
    "            (x0, key), _ = lax.scan(update, (x0, key), None, length=burn_in)\n",
    "            (_, _), xs = lax.scan(update, (x0, key), None, length=n_samples * thinning)\n",
    "            return xs[thinning - 1::thinning]\n",
    "\n",
    "        return _sample_impl(x0, self.key, n_samples, burn_in, thinning)\n",
    "\n",
    "class TruncatedGaussianHMCSampler(TruncatedGaussianSamplerBase):\n",
    "    def __init__(self, mean, cov, lower, upper, step_size=0.1, n_steps=10, key=None):\n",
    "        super().__init__(mean, cov, lower, upper, key)\n",
    "        self.step_size = step_size\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def sample(self, n_samples, burn_in=100, x0=None):\n",
    "        if x0 is None:\n",
    "            x0 = self._initial_point()\n",
    "\n",
    "        def _hmc_step(x, key):\n",
    "            key, p_key = random.split(key)\n",
    "            p = random.normal(p_key, (self.d,))\n",
    "            def leapfrog(carry, _):\n",
    "                x, p = carry\n",
    "                p = p - 0.5 * self.step_size * (self.P @ (x - self.mean))\n",
    "                x = x + self.step_size * p\n",
    "                x = jnp.where(x < self.lower, 2 * self.lower - x, x)\n",
    "                x = jnp.where(x > self.upper, 2 * self.upper - x, x)\n",
    "                p = jnp.where((x < self.lower) | (x > self.upper), -p, p)\n",
    "                p = p - 0.5 * self.step_size * (self.P @ (x - self.mean))\n",
    "                return (x, p), None\n",
    "            (x_new, _), _ = lax.scan(leapfrog, (x, p), None, length=self.n_steps)\n",
    "            return x_new, key\n",
    "\n",
    "        @partial(jit, static_argnums=(2,))\n",
    "        def _sample_impl(x0, key, n_samples):\n",
    "            def body(carry, _):\n",
    "                x, key = carry\n",
    "                x, key = _hmc_step(x, key)\n",
    "                return (x, key), x\n",
    "            (x0, key), _ = lax.scan(body, (x0, key), None, length=burn_in)\n",
    "            (_, _), xs = lax.scan(body, (x0, key), None, length=n_samples)\n",
    "            return xs\n",
    "\n",
    "        return _sample_impl(x0, self.key, n_samples)\n",
    "\n",
    "class TruncatedGaussianRejectionSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, x0=None):\n",
    "        def one_sample(key):\n",
    "            z = random.normal(key, (self.d,))\n",
    "            return self.mean + self.L @ z\n",
    "        return vmap(one_sample)(random.split(self.key, n_samples))\n",
    "\n",
    "class TruncatedGaussianImportanceSampler(TruncatedGaussianSamplerBase):\n",
    "    def sample(self, n_samples, x0=None):\n",
    "        def one_sample(key):\n",
    "            z = random.normal(key, (self.d,))\n",
    "            x = self.mean + self.L @ z\n",
    "            w = 1.0  # placeholder\n",
    "            return x, w\n",
    "        return vmap(one_sample)(random.split(self.key, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baba71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_constraint_coef(Smile_TIV):\n",
    "\n",
    "    first_derivative = Smile_TIV.pred_deriv['first_derivative']\n",
    "    second_derivative = Smile_TIV.pred_deriv['second_derivative']\n",
    "    function_val = Smile_TIV.pred_deriv['function_val']\n",
    "\n",
    "    b, a = Smile_TIV.m_scaler.inverse(1), Smile_TIV.m_scaler.inverse(0)\n",
    "\n",
    "    knots = Smile_TIV.m_scaler.inverse(Smile_TIV.x_test_extra)\n",
    "    knots_in_m = Smile_TIV.x_test_extra\n",
    "\n",
    "    B = np.zeros(len(knots))\n",
    "\n",
    "    for i in range(len(knots)):\n",
    "\n",
    "        comp_1 = knots_in_m[i]**2 * first_derivative[i] / ((b-a)* 4* function_val[i]**2) \n",
    "        comp_2 = -knots_in_m[i] / function_val[i]\n",
    "        comp_3 = -first_derivative[i] / ( (b-a)* 4* function_val[i])\n",
    "        comp_4 = -first_derivative[i] / ( (b-a)* 16)\n",
    "        \n",
    "        B[i] = (comp_1 + comp_2 + comp_3 + comp_4)/(b-a)\n",
    "    \n",
    "    C = 1/(2*(b-a)**2)\n",
    "        \n",
    "    return B,C\n",
    "\n",
    "\n",
    "B, C = _calculate_constraint_coef(Smile_TIV)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C = _calculate_constraint_coef(Smile_TIV)\n",
    "\n",
    "Lambda = np.zeros((m, m))\n",
    "for i in range(m):\n",
    "   if i != 0 and i != m-1:        \n",
    "        Lambda[i, i-1] = -B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "        Lambda[i, i+1] = B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "        Lambda[i, i] = -2*C / (delta_m**2)\n",
    "        \n",
    "   if i == 0:\n",
    "         Lambda[i, i+1] = B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "         Lambda[i, i] = -C / (delta_m**2)\n",
    "   if i == m-1:\n",
    "         Lambda[i, i-1] = -B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "         Lambda[i, i] = -C / (delta_m**2)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_samples(ax, samples, lower, upper, title):\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], s=10, alpha=0.5)\n",
    "    ax.set_xlim(lower[0] - 0.2, upper[0] + 0.2)\n",
    "    ax.set_ylim(lower[1] - 0.2, upper[1] + 0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "def gaussian_contour(ax, mean, cov):\n",
    "    # plot 1- and 2-sigma ellipses\n",
    "    from matplotlib.patches import Ellipse\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    for nsig in [1, 2]:\n",
    "        width, height = 2 * nsig * np.sqrt(vals)\n",
    "        angle = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "        ell = Ellipse(xy=mean, width=width, height=height,\n",
    "                      angle=angle, edgecolor='k', facecolor='none', lw=1)\n",
    "        ax.add_patch(ell)\n",
    "\n",
    "def main():\n",
    "    # Problem setup\n",
    "    mean = jnp.array([0.0, 0.0])\n",
    "    cov = jnp.array([[1.0, 0.8],\n",
    "                     [0.8, 1.0]])\n",
    "    lower = jnp.array([-2.0, -1.5])\n",
    "    upper = jnp.array([ 0.5,  1.2])\n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Instantiate each sampler\n",
    "    samplers = {\n",
    "        \"Gibbs\":       TruncatedGaussianGibbsSampler(mean, cov, lower, upper),\n",
    "        \"GHK\":         TruncatedGaussianGHKSampler(mean, cov, lower, upper),\n",
    "        \"Minimax\":     TruncatedGaussianMinimaxTiltingSampler(mean, cov, lower, upper),\n",
    "        \"Slice\":       TruncatedGaussianSliceSampler(mean, cov, lower, upper),\n",
    "        \"HMC\":         TruncatedGaussianHMCSampler(mean, cov, lower, upper, step_size=0.1, n_steps=15),\n",
    "        \"Rejection\":   TruncatedGaussianRejectionSampler(mean, cov, lower, upper),\n",
    "        \"Importance\":  TruncatedGaussianImportanceSampler(mean, cov, lower, upper),\n",
    "    }\n",
    "    \n",
    "    # Draw samples\n",
    "    samples_dict = {}\n",
    "    for name, sampler in samplers.items():\n",
    "        key, sub = random.split(key)\n",
    "        if name in [\"Gibbs\", \"Slice\"]:\n",
    "            samp = sampler.sample(n_samples, burn_in=200, thinning=2)\n",
    "        elif name == \"HMC\":\n",
    "            samp = sampler.sample(n_samples, burn_in=200)      # â† no thinning\n",
    "        elif name == \"Minimax\":\n",
    "            samp = sampler.sample(n_samples)\n",
    "        elif name in [\"GHK\", \"Rejection\"]:\n",
    "            samp = sampler.sample(n_samples)\n",
    "        else:  # Importance\n",
    "            samp, weights = sampler.sample(n_samples)\n",
    "            samp = np.array(samp)\n",
    "        samples_dict[name] = np.array(samp)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, (name, samp) in zip(axes, samples_dict.items()):\n",
    "        plot_samples(ax, samp, np.array(lower), np.array(upper), name)\n",
    "        gaussian_contour(ax, np.array(mean), np.array(cov))\n",
    "    \n",
    "    # Turn off unused subplot\n",
    "    axes[-1].axis('off')\n",
    "    fig.suptitle(\"2D Truncated Gaussian: Samples from Different Samplers\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f7423",
   "metadata": {},
   "source": [
    "# Sampling Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class TruncatedMVNSampler:\n",
    "    def __init__(self, mu, Sigma, a, b):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.Sigma = np.asarray(Sigma)\n",
    "        self.a = np.asarray(a)\n",
    "        self.b = np.asarray(b)\n",
    "        self.d = len(mu)\n",
    "\n",
    "        assert self.Sigma.shape == (self.d, self.d), \"Covariance matrix shape mismatch\"\n",
    "        assert self.a.shape == (self.d,) and self.b.shape == (self.d,), \"Bounds shape mismatch\"\n",
    "\n",
    "        # Cholesky decomposition of covariance matrix\n",
    "        self.L = np.linalg.cholesky(self.Sigma)\n",
    "\n",
    "        # Transform bounds to standard normal space\n",
    "        self.a_std = np.linalg.solve(self.L, self.a - self.mu)\n",
    "        self.b_std = np.linalg.solve(self.L, self.b - self.mu)\n",
    "\n",
    "    def transform_to_original_space(self, z):\n",
    "        return self.L @ z + self.mu\n",
    "\n",
    "    def log_tilted_density(self, z, theta):\n",
    "        return -0.5 * np.dot(z, z) + np.dot(theta, z)\n",
    "\n",
    "    def grad_cgf(self, theta, num_samples=10000):\n",
    "        z = np.random.randn(num_samples, self.d)\n",
    "        weights = np.exp(z @ theta - 0.5 * np.sum(z**2, axis=1))\n",
    "        return (weights[:, None] * z).mean(axis=0) / weights.mean()\n",
    "\n",
    "    def hess_cgf(self, theta, num_samples=10000):\n",
    "        z = np.random.randn(num_samples, self.d)\n",
    "        weights = np.exp(z @ theta - 0.5 * np.sum(z**2, axis=1))\n",
    "        Z_bar = (weights[:, None] * z).mean(axis=0) / weights.mean()\n",
    "        ZZ_bar = (weights[:, None, None] * np.einsum('ni,nj->nij', z, z)).mean(axis=0) / weights.mean()\n",
    "        return ZZ_bar - np.outer(Z_bar, Z_bar)\n",
    "\n",
    "    def find_optimal_tilting(self):\n",
    "        def objective(theta):\n",
    "            grad = self.grad_cgf(theta)\n",
    "            return 0.5 * np.dot(grad, grad)\n",
    "\n",
    "        def grad_objective(theta):\n",
    "            return self.hess_cgf(theta) @ self.grad_cgf(theta)\n",
    "\n",
    "        theta0 = np.zeros(self.d)\n",
    "        res = minimize(objective, theta0, jac=grad_objective, method='BFGS')\n",
    "        return res.x\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        theta_star = self.find_optimal_tilting()\n",
    "        accepted = []\n",
    "\n",
    "        while len(accepted) < n_samples:\n",
    "            z = np.random.randn(self.d)\n",
    "            log_w = -np.dot(theta_star, z) + 0.5 * np.dot(theta_star, theta_star)\n",
    "            if np.all(z >= self.a_std) and np.all(z <= self.b_std):\n",
    "                u = np.random.rand()\n",
    "                if np.log(u) < -log_w:\n",
    "                    accepted.append(self.transform_to_original_space(z))\n",
    "\n",
    "        return np.array(accepted) if n_samples > 1 else accepted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016969ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946006/464037173.py:11: RuntimeWarning: invalid value encountered in sqrt\n",
      "  std_devs = np.sqrt(np.diag(cov))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "params = constrained_gp.eta_parameters\n",
    "\n",
    "mean = params['mean_eta']\n",
    "cov = params['cov_eta']\n",
    "lower = params['lower_eta']\n",
    "upper = params['upper_eta']\n",
    "\n",
    "std_devs = np.sqrt(np.diag(cov))\n",
    "marginal_masses = []\n",
    "\n",
    "for i in range(len(mean)):\n",
    "\tmu = mean[i]\n",
    "\tsigma = std_devs[i]\n",
    "\ta = (lower[i] - mu) / sigma\n",
    "\tb = (upper[i] - mu) / sigma\n",
    "\tprob_mass = norm.cdf(b) - norm.cdf(a)\n",
    "\tmarginal_masses.append(prob_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerr = TruncatedMVnGHKTiltedSampler2(mean,\n",
    "                              cov+ np.eye(len(mean))*1e-3,\n",
    "                              lower,\n",
    "                              upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class TruncatedMVnGHKTiltedSampler2:\n",
    "\tdef __init__(self, mu, Sigma, a, b):\n",
    "\t\tself.mu = np.asarray(mu)\n",
    "\t\tself.Sigma = np.asarray(Sigma)\n",
    "\t\tself.a = np.asarray(a)\n",
    "\t\tself.b = np.asarray(b)\n",
    "\t\tself.d = len(mu)\n",
    "\n",
    "\t\tassert self.Sigma.shape == (self.d, self.d), \"Covariance matrix shape mismatch\"\n",
    "\t\tassert self.a.shape == (self.d,) and self.b.shape == (self.d,), \"Bounds shape mismatch\"\n",
    "\n",
    "\t\t# Cholesky decomposition of covariance matrix\n",
    "\t\tself.L = np.linalg.cholesky(self.Sigma)\n",
    "\n",
    "\t\t# Transform bounds to standard normal space\n",
    "\t\tself.a_std = np.linalg.solve(self.L, self.a - self.mu)\n",
    "\t\tself.b_std = np.linalg.solve(self.L, self.b - self.mu)\n",
    "\n",
    "\tdef grad_cgf(self, theta, num_samples=10000):\n",
    "\t\tz = np.random.randn(num_samples, self.d)\n",
    "\t\tweights = np.exp(z @ theta - 0.5 * np.sum(z**2, axis=1))\n",
    "\t\treturn (weights[:, None] * z).mean(axis=0) / weights.mean()\n",
    "\n",
    "\tdef hess_cgf(self, theta, num_samples=10000):\n",
    "\t\tz = np.random.randn(num_samples, self.d)\n",
    "\t\tweights = np.exp(z @ theta - 0.5 * np.sum(z**2, axis=1))\n",
    "\t\tZ_bar = (weights[:, None] * z).mean(axis=0) / weights.mean()\n",
    "\t\tZZ_bar = (weights[:, None, None] * np.einsum('ni,nj->nij', z, z)).mean(axis=0) / weights.mean()\n",
    "\t\treturn ZZ_bar - np.outer(Z_bar, Z_bar)\n",
    "\n",
    "\tdef find_optimal_tilting(self):\n",
    "\t\tdef objective(theta):\n",
    "\t\t\tgrad = self.grad_cgf(theta)\n",
    "\t\t\treturn 0.5 * np.dot(grad, grad)\n",
    "\n",
    "\t\tdef grad_objective(theta):\n",
    "\t\t\treturn self.hess_cgf(theta) @ self.grad_cgf(theta)\n",
    "\n",
    "\t\ttheta0 = np.zeros(self.d)\n",
    "\t\tres = minimize(objective, theta0, jac=grad_objective, method='BFGS')\n",
    "\t\treturn res.x\n",
    "\n",
    "\tdef sample(self, n_samples=1):\n",
    "\t\ttheta_star = self.find_optimal_tilting()\n",
    "\t\tprint(theta_star)\n",
    "\t\tsamples = []\n",
    "\t\tlog_weights = []\n",
    "\n",
    "\t\tfor _ in range(n_samples):\n",
    "\t\t\tz = np.zeros(self.d)\n",
    "\t\t\tlog_weight = 0.0\n",
    "\t\t\tfor i in range(self.d):\n",
    "\t\t\t\tLi = self.L[i, :i]\n",
    "\t\t\t\tmu_i = np.dot(Li, z[:i])\n",
    "\t\t\t\tstd_i = self.L[i, i]\n",
    "\n",
    "\t\t\t\tlower = (self.a_std[i] - mu_i) / std_i\n",
    "\t\t\t\tupper = (self.b_std[i] - mu_i) / std_i\n",
    "\n",
    "\t\t\t\ttheta_i = theta_star[i] * std_i\n",
    "\t\t\t\talpha = norm.cdf(upper - theta_i) - norm.cdf(lower - theta_i)\n",
    "\t\t\t\tprint(alpha)\n",
    "\t\t\t\tu = np.random.rand()\n",
    "\t\t\t\tquantile = norm.ppf(norm.cdf(lower - theta_i) + u * alpha)\n",
    "\t\t\t\tz_i = quantile + theta_i\n",
    "\n",
    "\t\t\t\tz[i] = mu_i + std_i * z_i\n",
    "\t\t\t\tlog_weight += -theta_star[i] * z_i + 0.5 * theta_star[i]**2\n",
    "\n",
    "\t\t\tx = self.L @ z + self.mu\n",
    "\t\t\tsamples.append(x)\n",
    "\t\t\tlog_weights.append(log_weight)\n",
    "\n",
    "\t\treturn np.array(samples) if n_samples > 1 else samples[0], np.exp(-np.array(log_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06192b",
   "metadata": {},
   "source": [
    "# Kaldiracaklar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable NumPy <-> R auto-conversion\n",
    "numpy2ri.activate()\n",
    "\n",
    "# Import the tmvtnorm package\n",
    "tmvtnorm = importr(\"tmvtnorm\")\n",
    "\n",
    "# Use your own pre-defined NumPy arrays:\n",
    "# mean, cov, lower, upper must already be defined\n",
    "\n",
    "# Wrap for R\n",
    "mean_r = FloatVector(constrained_gp.eta_parameters['mean_eta'].tolist())\n",
    "mod_r = FloatVector(constrained_gp.eta_parameters['mean_star_eta'].tolist())\n",
    "lower_r = FloatVector(constrained_gp.lower_bound.tolist())\n",
    "upper_r = FloatVector(constrained_gp.upper_bound.tolist())\n",
    "\n",
    "def to_r_matrix(array):\n",
    "\treturn robjects.r.matrix(FloatVector(array.flatten('F')), nrow=array.shape[0])\n",
    "\n",
    "new_cov = constrained_gp.eta_parameters['cov_eta'] + np.eye(constrained_gp.eta_parameters['cov_eta'].shape[0]) * 1e-6\n",
    "new_cov = 0.5 * (new_cov + new_cov.T)  # Ensure symmetry\n",
    "\n",
    "#new_cov += 1e-3 * np.eye(new_cov.shape[0])\n",
    "cov_r = to_r_matrix(cov_eta)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Call R function\n",
    "samples_r = tmvtnorm.rtmvnorm(\n",
    "\tn=n_samples,\n",
    "\tmean=mean_r,\n",
    "\tsigma=cov_r,\n",
    "\tlower=lower_r,\n",
    "\tupper=upper_r,\n",
    "\talgorithm = \"gibbs\",\n",
    "\t**{'start.value': mod_r}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_constraint_coef(Smile_TIV):\n",
    "\n",
    "    first_derivative = Smile_TIV.pred_deriv['first_derivative']\n",
    "    second_derivative = Smile_TIV.pred_deriv['second_derivative']\n",
    "    function_val = Smile_TIV.pred_deriv['function_val']\n",
    "\n",
    "    b, a = Smile_TIV.m_scaler.inverse(1), Smile_TIV.m_scaler.inverse(0)\n",
    "\n",
    "    knots = Smile_TIV.m_scaler.inverse(Smile_TIV.x_test_extra)\n",
    "    knots_in_m = Smile_TIV.x_test_extra\n",
    "\n",
    "    B = np.zeros(len(knots))\n",
    "\n",
    "    for i in range(len(knots)):\n",
    "\n",
    "        comp_1 = knots_in_m[i]**2 * first_derivative[i] / ((b-a)* 4* function_val[i]**2) \n",
    "        comp_2 = -knots_in_m[i] / function_val[i]\n",
    "        comp_3 = -first_derivative[i] / ( (b-a)* 4* function_val[i])\n",
    "        comp_4 = -first_derivative[i] / ( (b-a)* 16)\n",
    "        \n",
    "        B[i] = (comp_1 + comp_2 + comp_3 + comp_4)/(b-a)\n",
    "    \n",
    "    C = 1/(2*(b-a)**2)\n",
    "        \n",
    "    return B,C\n",
    "\n",
    "\n",
    "B, C = _calculate_constraint_coef(Smile_TIV)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C = _calculate_constraint_coef(Smile_TIV)\n",
    "\n",
    "Lambda = np.zeros((m, m))\n",
    "for i in range(m):\n",
    "   if i != 0 and i != m-1:        \n",
    "        Lambda[i, i-1] = -B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "        Lambda[i, i+1] = B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "        Lambda[i, i] = -2*C / (delta_m**2)\n",
    "        \n",
    "   if i == 0:\n",
    "         Lambda[i, i+1] = B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "         Lambda[i, i] = -C / (delta_m**2)\n",
    "   if i == m-1:\n",
    "         Lambda[i, i-1] = -B[i]/(2*delta_m) + C / (delta_m**2)\n",
    "         Lambda[i, i] = -C / (delta_m**2)\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
